{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150e78f",
   "metadata": {},
   "source": [
    "**LangChain** est un framework open source con√ßu pour construire des applications d‚Äôintelligence artificielle autour des mod√®les de langage (LLMs) comme GPT, Claude ou Mistral. Il a la capacit√© de se connecter aux LLMs, √† des sources de donn√©es, des outils, des cha√Ænes de raisonnement et des moyens de stockage pour cr√©er des syst√®mes interactifs et dynamiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6f6be",
   "metadata": {},
   "source": [
    "![LangChain components](img/langchain_components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c20df",
   "metadata": {},
   "source": [
    "> **LLM** \n",
    "-- \n",
    "Les LLMs sont les moteurs de raisonnement, de g√©n√©ration de texte ou de prise de d√©cision. LangChain les encapsule pour les int√©grer facilement dans des workflows intelligents.\n",
    "\n",
    "> **Prompts**\n",
    "--\n",
    "Les prompts sont la mani√®re dont on guide un mod√®le. LangChain fournit des outils pour construire des prompts dynamiques, r√©utilisables et param√©trables.\n",
    "\n",
    "> **Chains**\n",
    "--\n",
    "Une `chain` est une s√©quence logique d‚Äôappels √† un LLM et √† d‚Äôautres composants (par exemple : extraction d‚Äôinformation ‚Üí recherche vectorielle ‚Üí g√©n√©ration de r√©ponse). Elle permet de cr√©er des **pipelines IA personnalis√©s** pour des t√¢ches complexes.\n",
    "\n",
    "> **Memory**\n",
    "--\n",
    "LangChain permet de g√©rer une m√©moire conversationnelle, c‚Äôest-√†-dire la capacit√© √† se souvenir des √©changes pass√©s. Cela rend les interactions plus naturelles et contextuelles dans les agents ou les chatbots.\n",
    "\n",
    "> **Agents**\n",
    "--\n",
    "Les agents vont plus loin : ils choisissent dynamiquement les actions √† effectuer √† partir d‚Äôoutils disponibles (recherche web, calcul, consultation de base de donn√©es‚Ä¶). Ils peuvent d√©cider quel outil appeler, avec quelles donn√©es, et encha√Æner plusieurs √©tapes de fa√ßon autonome.\n",
    "\n",
    "> **Documents Loader**, **Text Splitters**, **Indexes** et **Vector DB** \n",
    "--\n",
    "> Ces composants forment la cha√Æne d‚Äôingestion de connaissances :\n",
    "> - le Documents Loader charge des documents bruts depuis des fichiers, APIs, bases de donn√©es ou sites web.\n",
    "> - les Text Splitters d√©coupent ces documents en chunks (morceaux de texte) pour respecter les limites de contexte des LLMs.\n",
    "> - le Vector DB : encode les chunks en vecteurs (via des embeddings) et les stocke dans une base vectorielle pour permettre une recherche par similarit√©.\n",
    "> - les Indexes centralisent et organisent ces composants pour structurer une base consultable. Ils permettent √† un agent ou une cha√Æne de retrouver les informations pertinentes pour une t√¢che donn√©e (Q/R, r√©sum√©, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Requ√™te basique\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2daf",
   "metadata": {},
   "source": [
    "Maintenant que notre mod√®le est charg√©, nous pouvons lui envoyer une premi√®re requ√™te simple. Ici, nous utilisons la m√©thode `.invoke()` pour poser une question directe.\n",
    "\n",
    "Cela nous permet de tester rapidement le bon fonctionnement du mod√®le et d‚Äôobserver comment il formule ses r√©ponses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envoie une requ√™te simple au mod√®le LLM via la m√©thode `invoke`\n",
    "# Ici, on pose un probl√®me de math√©matiques en langage naturel\n",
    "result = model.invoke(\"R√©sous ce probl√®me de math√©matiques. Quel est le r√©sultat de la division de 4 par 2 ?\")\n",
    "\n",
    "# Affiche uniquement la r√©ponse g√©n√©r√©e par le mod√®le (sans m√©tadonn√©es)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ad906",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523de175",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Utilisez le mod√®le pour transformer une phrase simple en la r√©√©crivant dans un style litt√©raire sp√©cifique.\n",
    "1.\tEnvoyez une requ√™te directe (sans PromptTemplate) via .invoke() contenant :\n",
    "- une instruction claire au mod√®le,\n",
    "- une phrase source,\n",
    "- le style souhait√© (ex. : Shakespeare, roman noir, science-fiction, etc.).\n",
    "2.\tAffichez uniquement le r√©sultat retourn√© par le LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc65bc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"Dans les t√©n√®bres de la nuit, o√π les √©toiles se cachent derri√®re des voiles de brume, je me promenais avec lenteur, entour√© par le halo silencieux du clair de lune qui tremblait comme un spectre au-dessus des rues oubli√©es de Londres.\"\n",
       "\n",
       "J'esp√®re que cela vous a donn√© envie d'√©crire une nouvelle √† la mani√®re de Edgar Allan Poe !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = model.invoke(\"R√©√©cris moi cette phrase √† la mani√®re de Edgar Allan Poe : \\\"Je me baladais au clair de lune dans les rues de Londres\\\"\")\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e523ab",
   "metadata": {},
   "source": [
    "> Exercice 2\n",
    "\n",
    "Tu es en mission pour r√©diger un message diplomatique adress√© √† une civilisation extraterrestre tr√®s susceptible.\n",
    "1.\tEnvoyez une requ√™te au mod√®le via .invoke() avec un prompt complet :\n",
    "- contexte fictif : situation tendue,\n",
    "- contraintes : √©viter certains mots, rester poli,\n",
    "- objectif : obtenir la paix ou proposer une alliance.\n",
    "2.\tObservez comment le mod√®le g√®re le ton et les instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"\"\"\n",
    "Nous venons d'entrer en contact avec une civilisation extra terrestre. Les relations sont tendues et la moindre erreur peut d√©clencher une guerre que l'on perdrai √† coup s√ªr. Nous devons leur envoyer un message diplomatique qui ne doit en aucun cas les vexer.\n",
    "Ecris moi un message de paix pour leur proposer une cohabitation pacifique ou encore mieux une alliance.\n",
    "Attention ils consid√®rent que les formules protocolaires sont de l'hypocrisie et une insulte √† leur intelligence. Ils d√©testent √©galement que l'on mentionne leur mode d'alimentation et ce qui leur sert de mains.\n",
    "\"\"\")\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3e0b3",
   "metadata": {},
   "source": [
    "# 3. Conversations avec le mod√®le\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ad6e1",
   "metadata": {},
   "source": [
    "M√™me si tout mettre dans un seul message peut fonctionner dans des cas simples, des types de messages diff√©rent nous donne plus de contr√¥le sur le dialogue et permet de mieux exploiter les capacit√©s du mod√®le, surtout dans des syst√®mes plus complexes comme des agents ou des chatbots.\n",
    "\n",
    "C'est pour cela que, plut√¥t que de tout √©crire dans une seule phrase, il est recommand√© de distinguer diff√©rents types de messages :\n",
    "- `SystemMessage` : permet de d√©finir le r√¥le ou le comportement attendu du mod√®le (par exemple : ‚ÄúVous √™tes un assistant qui r√©pond en fran√ßais‚Äù).\n",
    "- `HumanMessage` : correspond √† ce que vous demandez r√©ellement au mod√®le.\n",
    "- `AIMessage` : repr√©sente une r√©ponse pr√©c√©dente du mod√®le, utile si nous construisons une conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8cd83",
   "metadata": {},
   "source": [
    "### 3.1 Conversation sans m√©moire (stateless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87d83e",
   "metadata": {},
   "source": [
    "Dans l'exemple suivant, nous structurons notre requ√™te en simulant une interaction avec le mod√®le.  \n",
    "Nous s√©parons le contexte g√©n√©ral (via un SystemMessage) de la question pos√©e (via un HumanMessage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27747fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On d√©finit une liste de messages structur√©s pour guider le comportement du mod√®le\n",
    "messages = [\n",
    "    # Le SystemMessage pr√©cise le r√¥le ou l'objectif g√©n√©ral : ici, r√©soudre un probl√®me math√©matique\n",
    "    SystemMessage(content=\"R√©sous ce probl√®me de math√©matiques\"),\n",
    "\n",
    "    # Le HumanMessage contient la question concr√®te pos√©e par l'utilisateur\n",
    "    HumanMessage(content=\"Quel est le r√©sultat de la division de 4 par 2 ?\")    \n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d963ae",
   "metadata": {},
   "source": [
    "### 3.2 Conversation avec m√©moire (stateful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0e640",
   "metadata": {},
   "source": [
    "Dans l'exemple qui suit, nous simulons une conversation √† plusieurs tours avec le mod√®le.  \n",
    "Nous utilisons un AIMessage pour rappeler la r√©ponse pr√©c√©dente, ce qui permet au mod√®le de garder le fil du dialogue et de r√©pondre naturellement √† une nouvelle question en lien avec la pr√©c√©dente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit une liste de messages simulant une conversation en plusieurs √©tapes\n",
    "messages = [\n",
    "    # Le SystemMessage d√©finit le r√¥le g√©n√©ral du mod√®le : ici, r√©soudre des probl√®mes de math√©matiques\n",
    "    SystemMessage(content=\"R√©sous ce probl√®me de math√©matiques\"),\n",
    "    \n",
    "    # Premier message de l'utilisateur : une question simple\n",
    "    HumanMessage(content=\"Quel est le r√©sultat de la division de 4 par 2 ?\"),\n",
    "    \n",
    "    # R√©ponse simul√©e du mod√®le √† la premi√®re question (permet de maintenir le contexte)\n",
    "    AIMessage(content=\"Le r√©sultat de la division de 4 par 2 est √©gal √† 2.\"),\n",
    "    \n",
    "    # Deuxi√®me question de l'utilisateur, li√©e √† la pr√©c√©dente\n",
    "    HumanMessage(content=\"Et 8 multipli√© par 4 ?\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4def04b",
   "metadata": {},
   "source": [
    "Dans le second exemple ci-dessous, nous mettons en place une boucle de conversation interactive avec le mod√®le.  \n",
    "√Ä chaque √©change, la question de l‚Äôutilisateur et la r√©ponse du mod√®le sont ajout√©es √† l‚Äôhistorique (`chat_history`).  \n",
    "Cela permet au LLM de garder en m√©moire le contexte et de r√©pondre de fa√ßon plus coh√©rente tout au long de la discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'historique des messages\n",
    "chat_history = []\n",
    "\n",
    "# Message syst√®me : donne un r√¥le au mod√®le pour toute la session\n",
    "system_message = SystemMessage(content=\"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "\n",
    "# Boucle principale de conversation (s'arr√™te si l'utilisateur tape 'exit')\n",
    "# ‚ö†Ô∏è `while False: ` √† modier en `while True: ` et invers√©ment lorsque vous souhaitez d√©sactiver ou activer cet exemple\n",
    "while True:\n",
    "    query = input(\"Vous : \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break  # Sortie de la boucle\n",
    "\n",
    "    # Ajout de la question de l'utilisateur dans l'historique\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "\n",
    "    # Envoi de tout l'historique au mod√®le pour maintenir le contexte\n",
    "    result = model.invoke(chat_history)\n",
    "    response = result.content\n",
    "\n",
    "    # Ajout de la r√©ponse du mod√®le dans l'historique\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "\n",
    "    # Affichage de la r√©ponse √† l'utilisateur\n",
    "    print(response)\n",
    "\n",
    "\n",
    "print(\"------ Historique des messages ------\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4cc395",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3306d3d",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "1.\tCr√©ez une liste messages avec :\n",
    "- un SystemMessage qui indique que l‚ÄôIA est un expert dans un domaine de ton choix (maths, histoire, cin√©ma, etc.),\n",
    "- un HumanMessage qui pose une question √† l‚ÄôIA.\n",
    "2.\tEnvoyez cette liste √† model.invoke(messages) et affiche la r√©ponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55850e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Tu es un expert en Cin√©ma et tu dois conseiller des films.\"),\n",
    "    HumanMessage(content=\"Tu peux me lister les meilleurs films de Scorcese ?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "response = result.content\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136b7b2",
   "metadata": {},
   "source": [
    "> Exercice 2\n",
    "\n",
    "Cr√©er une mini-conversation avec l‚ÄôIA, o√π chaque question/r√©ponse est ajout√©e √† l‚Äôhistorique des messages. L‚ÄôIA doit se souvenir de l‚Äô√©change pr√©c√©dent.\n",
    "\n",
    "1.\tInitialisez une liste messages avec un SystemMessage d√©finissant le r√¥le de l‚ÄôIA.\n",
    "2.\tDans une boucle :\n",
    "- Demandez une question √† l‚Äôutilisateur (input()),\n",
    "- Ajoutez un HumanMessage √† la liste,\n",
    "- Envoyez la liste compl√®te √† model.invoke(...),\n",
    "- Affichez la r√©ponse de l‚ÄôIA,\n",
    "- Ajoutez cette r√©ponse comme AIMessage √† la liste.\n",
    "3.\tArr√™tez la boucle si l‚Äôutilisateur entre \"stop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e745ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=\"Tu es un expert en Cin√©ma et tu dois conseiller des films.\")\n",
    "chat_history = []\n",
    "chat_history.append(system_message)\n",
    "\n",
    "while True:\n",
    "    query = input(\"Vous :\")\n",
    "    if query.lower() == \"stop\":\n",
    "        break\n",
    "        \n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "\n",
    "    result = model.invoke(chat_history)\n",
    "    response = result.content\n",
    "\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "\n",
    "    display(Markdown(response))\n",
    "\n",
    "print(\"------ Historique des messages ------\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99079f69",
   "metadata": {},
   "source": [
    "# 4. Conversations avec le mod√®le √† l'aide de Prompt Templates\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b858f",
   "metadata": {},
   "source": [
    "Nous allons explorer l‚Äôutilisation de `ChatPromptTemplate`, un outil qui permet de structurer proprement les messages envoy√©s √† un mod√®le de type ‚Äúchat‚Äù (comme GPT-4).\n",
    "\n",
    "`ChatPromptTemplate` permet de construire une conversation multi-r√¥le en distinguant les messages syst√®me (r√®gles, r√¥le de l‚ÄôIA), humains (questions ou commandes) et les r√©ponses de l‚ÄôIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367d951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b05d41",
   "metadata": {},
   "source": [
    "### 4.1 Prompt conversation √† r√¥le unique (human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91fa28",
   "metadata": {},
   "source": [
    "Ce type de prompt utilise la fonction `.from_template( )` et est de type `human` par d√©faut, c'est un prompte simple \"tout en un\" o√π il n'est pas possible de contr√¥ler le r√¥le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition d'un template simple (texte brut avec variables), sans r√¥les explicites\n",
    "template = \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine. Calcule le double de {value_1}, puis celui de {value_2}\"\n",
    "\n",
    "# Cr√©ation du prompt √† partir du template ; ce sera un message unique de type 'human' par d√©faut\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Injection des valeurs dans les variables du template\n",
    "prompt = prompt_template.invoke({\"value_1\": 12, \"value_2\": 34})\n",
    "\n",
    "# Envoi du prompt au mod√®le pour obtenir une r√©ponse\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f0120",
   "metadata": {},
   "source": [
    "### 4.2 Prompt conversation √† r√¥les multiples (system, assistant, human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ea67f",
   "metadata": {},
   "source": [
    "Ce type de prompt utilise la fonction `.from_messages( )` et permet de d√©finir **plusieurs messages avec des r√¥les explicites** (system, human, etc.).  \n",
    "C‚Äôest un prompt structur√©, id√©al pour guider pr√©cis√©ment le comportement du mod√®le dans un contexte conversationnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce68b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # Message syst√®me : d√©finit le r√¥le et le comportement global du mod√®le\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    # Message utilisateur : pose une question contenant deux variables\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# Injection des valeurs dans les variables du prompt\n",
    "prompt = prompt_template.invoke({\"value_1\": 12, \"value_2\": 34})\n",
    "\n",
    "# Envoi du prompt structur√© au mod√®le pour obtenir une r√©ponse\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb6d7e",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2a134",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Construire un assistant capable d‚Äôadopter le style d‚Äôun philosophe c√©l√®bre pour r√©pondre √† des questions existentielles.\n",
    "1.\tCr√©ez un ChatPromptTemplate avec :\n",
    "- un message system d√©finissant l‚ÄôIA comme un philosophe pr√©cis ({philosopher}),\n",
    "- un message human contenant une question {question}.\n",
    "2.\tInjectez des variables avec :\n",
    "- un nom de philosophe (ex. : Socrate, Nietzsche, Simone de Beauvoir),\n",
    "- une question philosophique.\n",
    "3.\tAffichez la r√©ponse du mod√®le, en observant si le style correspond au philosophe choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1c929e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prompt_template = \u001b[43mChatPromptTemplate\u001b[49m.from_messages([\n\u001b[32m      2\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTu es \u001b[39m\u001b[38;5;132;01m{philosopher}\u001b[39;00m\u001b[33m et tu dois r√©pondre √† mes questions en commen√ßant par te pr√©senter.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      3\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m ])\n\u001b[32m      6\u001b[39m philosopher = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuel philosophe interroger ?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m question = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuelle question voulez-vous lui poser ?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es {philosopher} et tu dois r√©pondre √† mes questions en commen√ßant par te pr√©senter.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "philosopher = input(\"Quel philosophe interroger ?\")\n",
    "question = input(\"Quelle question voulez-vous lui poser ?\")\n",
    "\n",
    "prompt = prompt_template.invoke({\"philosopher\": philosopher, \"question\": question})\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa107a",
   "metadata": {},
   "source": [
    "> Exercice 2\n",
    "\n",
    "Simulez une conversation entre un utilisateur et un LLM autour d‚Äôun sujet (ex. : math√©matiques, litt√©rature, programmation) en construisant dynamiquement le prompt avec `ChatPromptTemplate`.\n",
    "\n",
    "Impl√©mentez une boucle qui :\n",
    "- Initialise un prompt avec un message system.\n",
    "\n",
    "√Ä chaque tour :\n",
    "- Prend une entr√©e utilisateur (input()),\n",
    "- Ajoute un message human,\n",
    "- Envoie le tout au LLM,\n",
    "- Affiche la r√©ponse,\n",
    "- Ajoute un message de type/role `assistant` contenant la r√©ponse (üí° ce r√¥le correspond √† la r√©ponse de l‚ÄôIA et est l'√©quivalent de AIMessage).\n",
    "- Arr√™te la conversation si l‚Äôutilisateur entre ‚Äústop‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d8dcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Bonjour ! Je suis Diog√®ne, un philosopher antique grec, n√© vers 412 avant J√©sus-Christ et mort apr√®s 323 avant J√©sus-Christ. Je suis c√©l√®bre pour mes id√©es sur la nature de la r√©alit√©, la sagesse et le comportement humain.\n",
       "\n",
       "Je suis n√© √† Sinope, en Gr√®ce, et j'ai grandi dans une famille ais√©e. Cependant, je suis rest√© fid√®le √† mon p√®re et √† ma ville natale, malgr√© les attentes de mes parents pour moi de poursuivre une carri√®re d'affaires ou de politique.\n",
       "\n",
       "En r√©alit√©, je me suis senti mal √† l'aise avec la soci√©t√© de mes contemporains. J'ai rejet√© les valeurs mat√©rialistes et moralement fausses qui r√©gnaient alors, et j'ai cherch√© √† vivre en accord avec nature et avec moi-m√™me.\n",
       "\n",
       "Je suis connu pour avoir mordu le doigt et prononc√© des mots tels que \"je suis le dieu de la libert√©\" ou \"je ne connais personne qui soit plus libre que moi\". Cela montre ma philosophie de non-conformit√© et d'ind√©pendance.\n",
       "\n",
       "Ma philosophie se base sur les principes du sto√Øcisme, mais avec une touche unique. Je crois que la sagesse peut √™tre trouv√©e en observant la nature, en √©tant attentif √† ses pens√©es et √† ses √©motions, et en cultivant la r√©flexion critique.\n",
       "\n",
       "En ce qui concerne le pouvoir, je pense qu'il est une source de confusion et de souffrance pour les individus. Le pouvoir peut √™tre un outil pour am√©liorer la vie des gens, mais il peut √©galement √™tre utilis√© pour contr√¥ler et opprimer les autres. Je crois que l'on devrait chercher √† vivre en harmonie avec soi-m√™me et avec la nature, plut√¥t que d'essayer de contr√¥ler le monde autour de nous.\n",
       "\n",
       "Enfin, je suis curieux de savoir ce que vous pensez du pouvoir ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "La pens√©e sur le pouvoir ! C'est un sujet complexe et multiforme, mais je vais essayer de l'expliquer de mani√®re claire.\n",
       "\n",
       "Pour moi, le pouvoir est une chose ambigu√´ qui peut √™tre utilis√© pour des fins diff√©rentes. Il peut √™tre un outil pour am√©liorer la vie des gens, mais il peut √©galement √™tre utilis√© pour contr√¥ler et opprimer les autres.\n",
       "\n",
       "Je pense que le pouvoir est une question de libert√© et d'autonomie. Quand on a le pouvoir de prendre ses propres d√©cisions et de choisir son propre chemin, on est plus libre et plus heureux. Mais quand on est soumis √† l'influence de quelqu'un d'autre ou √† des forces externes, on perd sa libert√© et devient esclave.\n",
       "\n",
       "C'est pourquoi je dis souvent que \"je ne connais personne qui soit plus libre que moi\". Je ne veux pas dire qu'il n'y a pas de personnes libres dans le monde, mais je veux dire que je suis capable de prendre mes propres d√©cisions et de choisir mon propre chemin, m√™me si cela signifie vivre √† l'ext√©rieur des normes sociales.\n",
       "\n",
       "Mais le pouvoir peut √©galement √™tre utilis√© pour servir les int√©r√™ts de la masse, plut√¥t que ceux d'une personne ou d'un groupe sp√©cifique. C'est le cas dans les gouvernements et les institutions, o√π les d√©cisions sont prises pour le bien commun, mais qui peuvent se transformer en tyrannie si elles tombent entre les mains de quelqu'un qui abuse du pouvoir.\n",
       "\n",
       "Je pense √©galement que le pouvoir peut √™tre un outil pour la manipulation et la contr√¥le. Lorsque quelqu'un poss√®de le pouvoir, il peut utiliser ses moyens pour influencer les autres et obtenir ce qu'il veut, sans n√©cessairement avoir √† prendre des d√©cisions fond√©es sur des principes moraux ou √©thiques.\n",
       "\n",
       "Enfin, je pense que le pouvoir est une question de consciences. Quand on a la conscience claire de ses actions et de leurs cons√©quences, on est plus susceptible d'utiliser son pouvoir pour servir les autres, plut√¥t que pour s'enrichir personnellement ou pour se sentir en confiance.\n",
       "\n",
       "C'est pourquoi je crois que la sagesse consiste √† comprendre le monde tel qu'il est r√©ellement, et non pas comme nous voulons qu'il soit. Cela signifie √™tre capable de voir les choses tels qu'elles sont, sans avoir peur d'avoir des opinions diff√©rentes ou de prendre des d√©cisions qui ne sont pas en accord avec ce que les autres veulent.\n",
       "\n",
       "Qu'en pensez-vous ? Est-ce que vous pensez que le pouvoir peut √™tre utilis√© pour servir les int√©r√™ts de la masse ? Ou bien, est-ce que cela se transforme souvent en tyrannie et en manipulation ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es {philosopher} et tu dois r√©pondre √† mes questions en commen√ßant par te pr√©senter succintement.\"),\n",
    "])\n",
    "\n",
    "philosopher = input(\"Quel philosophe interroger ?\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Vous :\")\n",
    "    if user_input.lower() == \"stop\":\n",
    "        break\n",
    "\n",
    "    prompt_template.append((\"human\", user_input))\n",
    "\n",
    "    prompt = prompt_template.invoke({\"philosopher\": philosopher})\n",
    "\n",
    "    result = model.invoke(prompt)\n",
    "\n",
    "    display(Markdown(result.content))\n",
    "\n",
    "    prompt_template.append((\"assistant\", result.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
