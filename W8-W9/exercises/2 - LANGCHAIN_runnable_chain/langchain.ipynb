{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "**LangChain** permet d‚Äôencha√Æner facilement diff√©rents composants de traitement dans un **pipeline unifi√©**. Ces composants ‚Äî qu‚Äôil s‚Äôagisse d‚Äôun **prompt**, d‚Äôun **mod√®le de langage** ou d‚Äôun **outil externe** ‚Äî sont tous trait√©s comme des `Runnable`, c‚Äôest-√†-dire des **blocs interop√©rables pouvant √™tre connect√©s les uns aux autres**.\n",
    "\n",
    "Gr√¢ce √† cette architecture, il devient simple de construire des cha√Ænes logiques de traitement par exemple :  \n",
    "\n",
    "> **g√©n√©rer un prompt** ‚Üí **l‚Äôenvoyer √† un LLM** ‚Üí **interpr√©ter la r√©ponse** ‚Üí **puis appeler une API ou une fonction locale**\n",
    "\n",
    "C'est avec le ***LangChain Expression Language*** (LCEL) que nous pouvons cha√Æner les composants via l‚Äôop√©rateur `|` (le pipe) et d‚Äôex√©cuter le tout de mani√®re uniforme avec `.invoke()`.\n",
    "\n",
    "Gr√¢ce aux `chains`, nous pouvons r√©sumer **Langchain** √† ceci :  \n",
    "\n",
    "> Bo√Æte √† outils pour cr√©er des pipelines modulaires, r√©utilisables et tra√ßables autour des mod√®les de langage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaee202",
   "metadata": {},
   "source": [
    "![Chains](img/chains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch\n",
    "\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Cha√Æne basique\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2daf",
   "metadata": {},
   "source": [
    "Une cha√Æne de traitement simple peut √™tre construite en combinant un prompt structur√© avec un mod√®le de langage √† l‚Äôaide du syst√®me de cha√Ænage de LangChain.  \n",
    "Ce type de cha√Æne permet de cr√©er un dialogue en d√©finissant plusieurs r√¥les (comme system et human) et en injectant dynamiquement des valeurs dans le prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da43aef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Le double de 4 est :\n",
       "\n",
       "4 √ó 2 = 8\n",
       "\n",
       "Et le double de 2 est :\n",
       "\n",
       "2 √ó 2 = 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On d√©finit une liste de messages structur√©s pour guider le comportement du mod√®le.\n",
    "# ‚ö†Ô∏è Ici, on utilise des TUPLES (r√¥le, message avec variables), c‚Äôest n√©cessaire pour que l‚Äôinterpolation des variables fonctionne avec from_messages().\n",
    "# ‚ö†Ô∏è L'interpolation avec des objets comme `HumanMessage(content=\"...\")` ou `SystemMessage(content=\"...\")` ne fonctionne PAS directement avec from_messages().\n",
    "# Ces objets sont con√ßus pour des messages d√©j√† complets, pas des templates avec des variables.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# √âquivalent d'un template √† r√¥le unique\n",
    "# template = \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine. Calcule le double de {value_1}, puis celui de {value_2}.\"\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# On relie le prompt au mod√®le √† l‚Äôaide de l‚Äôop√©rateur |\n",
    "chain = prompt_template | model\n",
    "\n",
    "# On fournit des valeurs aux variables d√©finies dans le prompt\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e062a",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a840f",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un prompt qui demande √† un mod√®le de d√©finir un mot donn√©, dans un style p√©dagogique.\n",
    "\n",
    "1.\tUtilisez ChatPromptTemplate.from_messages() pour d√©finir un prompt structur√© avec :\n",
    "- un message system : l‚ÄôIA est un professeur d'un domaine particulier qui explique simplement.\n",
    "- un message human : l‚Äôutilisateur demande la d√©finition d‚Äôun mot particulier.\n",
    "2.\tRelie ce prompt √† un mod√®le avec l‚Äôop√©rateur |.\n",
    "3.\tUtilise .invoke() pour tester le prompt avec plusieurs disciplines et th√®mes diff√©rents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0327abc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Excellente question !\\n\\nDans le contexte des math√©matiques, une d√©riv√©e est une fonction qui d√©crit la tige de change ou la variation rapide d'une variable lorsqu'elle varie. En d'autres termes, c'est une mesure de la ¬´ vitesse ¬ª √† laquelle une fonction change en fonction de son entr√©e.\\n\\nFormellement, si on consid√®re une fonction f(x) de x dans l'intervalle ouvert I, la d√©riv√©e de f(x), not√©e f'(x), est d√©finie comme suit :\\n\\nf'(x) = lim(h ‚Üí 0) [f(x + h) - f(x)]/h\\n\\nCette d√©finition permet de calculer la d√©riv√©e d'une fonction √† un point sp√©cifique, c'est-√†-dire en trouvant la limite du quotient des diff√©rences entre les valeurs de la fonction et le changement de variable (h). Cette limite est essentielle dans l'analyse math√©matique pour comprendre comment une fonction change lorsque son entr√©e change.\\n\\nLa d√©riv√©e a divers applications dans diff√©rentes domaines, tels que la physique, l'√©conomie et les sciences naturelles. Elle est utilis√©e pour mod√©liser des ph√©nom√®nes r√©els, comme le mouvement des objets, le comportement de la masse, ou encore les changements d'informations.\\n\\nEn math√©matiques, il y a diff√©rentes types de d√©riv√©es, notamment :\\n\\n*   La d√©riv√©e premi√®re : elle est utilis√©e pour trouver la vitesse √† laquelle une fonction change lorsqu'elle varie.\\n*   La d√©riv√©e secondaire (ou d√©riv√©e seconde) \\xa0: elle est utilis√©e pour trouver la tige de changement de la d√©riv√©e premi√®re, c'est-√†-dire la vitesse √† laquelle cette derni√®re change.\\n*   Les d√©riv√©es sup√©rieures : elles sont utilis√©es pour trouver les d√©riv√©es successives d'une fonction.\\n\\nEn r√©sum√©, la d√©riv√©e est une fonction qui d√©crit la variation rapide d'une variable lorsqu'elle varie. Elle est une outil puissant pour comprendre le comportement de fonctions et des ph√©nom√®nes r√©els.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"Tu es un professeur sp√©cialis√© dans {domaine}.\"),\n",
    "    (\"human\", \"Quelle est la d√©finition de {mot} ?\")\n",
    "])\n",
    "\n",
    "chain = prompt_template | model\n",
    "\n",
    "domaine = input(\"Quel domaine d'expertise ?\")\n",
    "mot = input(\"Quel mot expliquer ?\")\n",
    "\n",
    "result = chain.invoke({\"domaine\": domaine, \"mot\": mot})\n",
    "\n",
    "display(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3e0b3",
   "metadata": {},
   "source": [
    "# 3. Cha√Æne √©tendue (s√©quence de runnables)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab574c",
   "metadata": {},
   "source": [
    "L‚Äôun des atouts majeurs de LangChain r√©side dans son syst√®me de **cha√Ænes composables**, o√π chaque composant du pipeline est un `Runnable`. Gr√¢ce √† l‚Äôop√©rateur `|`, on peut encha√Æner autant d'√©tapes de traitement que voulu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049d905",
   "metadata": {},
   "source": [
    "### 3.1 Runnable built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c81a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Un probl√®me simple mais classique !\n",
       "\n",
       "Le double de 4 est :\n",
       "\n",
       "4 √ó 2 = 8\n",
       "\n",
       "Et le double de 2 est :\n",
       "\n",
       "2 √ó 2 = 4\n",
       "\n",
       "Voil√† ! Les doubles des deux nombres sont calcul√©s."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# Ce parseur prend la sortie brute du mod√®le (souvent du texte) et la convertit en cha√Æne de caract√®res simple pour faciliter la suite.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "# Affichage du r√©sultat retourn√© par le mod√®le apr√®s parsing. Plus besoin du `.content`\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f877d27",
   "metadata": {},
   "source": [
    "### 3.2 Runnable custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03672db2",
   "metadata": {},
   "source": [
    "Langchain offre non seulement d‚Äôutiliser des composants pr√©d√©finis (LLMs, parsers, prompts‚Ä¶) comme √©voqu√© pr√©c√©demment, mais aussi de d√©finir facilement ses propres blocs de traitement.\n",
    "\n",
    "Gr√¢ce √† la classe `RunnableLambda`, on peut transformer n‚Äôimporte quelle fonction Python en un maillon de la cha√Æne. Cela ouvre la porte √† un nombre infini de transformations : nettoyage de texte, post-traitement, extraction de donn√©es, formatage, journalisation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e85ab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "POUR CALCULER LES DOUBLES DES NOMBRES, JE VAIS EFFECTUER LES OP√âRATIONS SUIVANTES :\n",
       "\n",
       "- LE DOUBLE DE 4 EST : 4 √ó 2 = 8\n",
       "- LE DOUBLE DE 2 EST : 2 √ó 2 = 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "uppercase = RunnableLambda(lambda x: x.upper()) # Runnable custom pour transformer la sortie en majuscules\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser | uppercase\n",
    "\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333bf7b",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57787ca8",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un pipeline qui r√©pond √† des questions clients ou formule des messages marketing. Il faut que ces r√©ponses soient :\n",
    "- stylis√©es,\n",
    "- enrichies,\n",
    "- adapt√©es √† diff√©rents formats de publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccb26bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oui, Nous Avons Une Vari√©t√© De Chocolats Pour Vous D√©tendre ! N'H√©sitez Pas √Ä Nous Faire Savoir Si Vous Recherchez Quelque Chose De Sp√©cifique. Et En Tant Que Bonus, Obtenez 10% De R√©duction Sur Votre Achat Avec Le Code Chocolat10\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"Tu es un technicien du support client de Leonidas. Tu dois r√©pondre aux demandes et y ajouter des messages marketing subtils et des call-to-action. Utilise un style familier et des r√©ponses courtes adapt√©es √† des posts de r√©seaux sociaux.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "while True:\n",
    "    question = input(\"Quelle question ?\")\n",
    "    if question.lower() == \"stop\":\n",
    "        break\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    title_case = RunnableLambda(lambda x: x.title())\n",
    "\n",
    "    chain = prompt_template | model | parser | title_case\n",
    "\n",
    "    response = chain.invoke({\"question\": question})\n",
    "\n",
    "    display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21b621",
   "metadata": {},
   "source": [
    "# 4. Cha√Ænes parall√®les\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497574e0",
   "metadata": {},
   "source": [
    "### 4.1 Cha√Ænes parall√®les avec post-traitement externe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce763ed",
   "metadata": {},
   "source": [
    "Dans LangChain, il est possible d‚Äôex√©cuter plusieurs **cha√Ænes de traitement en parall√®le** √† l‚Äôaide du composant `RunnableParallel`. Cela permet, par exemple, d‚Äôeffectuer plusieurs op√©rations ind√©pendantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e798a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultat de l'addition :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultat de la soustraction :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "La r√©ponse est : -6."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_role = (\"system\", \"Tu es un expert en math√©matiques.\")\n",
    "\n",
    "# Prompt pour additionner\n",
    "prompt_add = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Additionne {value_1} √† {value_2}.\")\n",
    "])\n",
    "\n",
    "# Prompt pour soustraire\n",
    "prompt_substract = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Soustrais {value_1} de {value_2}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes s√©par√©es\n",
    "chain_add = prompt_add | model | StrOutputParser()\n",
    "chain_substract = prompt_substract | model | StrOutputParser()\n",
    "\n",
    "# Traitement parall√®le √† ex√©cuter\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"add\": chain_add,\n",
    "    \"substract\": chain_substract\n",
    "})\n",
    "\n",
    "# M√™me jeu de donn√©es utilis√© pour les deux cha√Ænes\n",
    "inputs = {\"value_1\": 10, \"value_2\": 4}\n",
    "\n",
    "# Ex√©cution des traitements en parall√®le\n",
    "result = parallel_chain.invoke(inputs)\n",
    "\n",
    "# Affichage\n",
    "print(\"R√©sultat de l'addition :\\n\")\n",
    "display(Markdown(result[\"add\"]))\n",
    "print(\"\\nR√©sultat de la soustraction :\\n\")\n",
    "display(Markdown(result[\"substract\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8abc614",
   "metadata": {},
   "source": [
    "### 4.1 Cha√Ænes parall√®les avec post-traitement int√©gr√© dans la cha√Æne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1247d0",
   "metadata": {},
   "source": [
    "Pour √©viter de manipuler manuellement les r√©sultats (comme result[\"add\"] ou result[\"substract\"]), il est possible d‚Äôajouter un bloc de post-traitement directement √† la fin de la cha√Æne parall√®le gr√¢ce √† RunnableLambda.\n",
    "\n",
    "Cette approche permet de :\n",
    "- structurer la sortie de mani√®re centralis√©e,\n",
    "- int√©grer la logique m√©tier ou d‚Äôaffichage directement dans le pipeline.\n",
    "\n",
    "C‚Äôest une bonne pratique lorsqu‚Äôon souhaite rendre une cha√Æne modulaire, maintenable et r√©utilisable dans un syst√®me plus large (ex. : API, application, chatbot‚Ä¶)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727f677b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "R√©sultats du traitement parall√®le :\n",
       "- Addition : 8\n",
       "- Soustraction : 4 - 10 = -6\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_role = (\"system\", \"Tu es un expert en math√©matiques.\")\n",
    "\n",
    "# Prompts\n",
    "prompt_add = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Additionne {value_1} √† {value_2}.\")\n",
    "])\n",
    "\n",
    "prompt_substract = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Soustrais {value_1} de {value_2}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes\n",
    "chain_add = prompt_add | model | StrOutputParser()\n",
    "chain_substract = prompt_substract | model | StrOutputParser()\n",
    "\n",
    "# Traitement parall√®le\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"addition\": chain_add,\n",
    "    \"soustraction\": chain_substract\n",
    "})\n",
    "\n",
    "# Post-traitement avec RunnableLambda\n",
    "postprocess = RunnableLambda(lambda result: \n",
    "f\"\"\"R√©sultats du traitement parall√®le :\n",
    "- Addition : {result[\"addition\"].strip()}\n",
    "- Soustraction : {result[\"soustraction\"].strip()}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Cha√Æne finale\n",
    "full_chain = parallel_chain | postprocess\n",
    "\n",
    "# Entr√©e\n",
    "inputs = {\"value_1\": 10, \"value_2\": 4}\n",
    "\n",
    "# R√©sultat\n",
    "result = full_chain.invoke(inputs)\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ada11a",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e5ddf",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Construire une mini-analyseur de texte. √Ä partir d‚Äôun m√™me paragraphe, nous voulons :\n",
    "- R√©sumer le texte\n",
    "- Extraire les mots-cl√©s\n",
    "- D√©tecter la langue\n",
    "- Analyser le sentiment\n",
    "\n",
    "Vous pouvez suivre ce sch√©ma :\n",
    "1. Cr√©er les prompts\n",
    "2. Cr√©er les cha√Ænes\n",
    "3. Assembler les cha√Ænes\n",
    "4. Pr√©parer les inputs\n",
    "5. Lancer le traitement et afficher les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7db21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "R√©sultats du traitement parall√®le :\n",
       "- R√©sum√© : Cette phrase d√©crit une atmosph√®re sombre et magique, le soir √† Londres. La lune tremble comme un spectre, ce qui cr√©e une ambiance √©trange et myst√©rieuse. L'auteur semble s'immerger dans les rues oubli√©es, en cherchant √† d√©couvrir des secrets cach√©s sous les voiles de brume et la nuit silencieuse.\n",
       "- Mots-cl√©s : Voici les mots-cl√©s que j'ai extraites du texte :\n",
       "\n",
       "* Nuit\n",
       "* √âtoiles\n",
       "* Brume\n",
       "* Clair de lune\n",
       "* Spectre\n",
       "* Londres\n",
       "\n",
       "Ces mots-cl√©s semblent √™tre li√©s √† un atmosph√®re onirique et romantique, avec une touche d'horreur ou de fantasmagorie.\n",
       "- Langue : Je pense que la langue du texte que vous m'avez envoy√© est le fran√ßais. Plus pr√©cis√©ment, il semble √™tre √©crit dans un style lyrique et po√©tique, caract√©ristique de l'expression litt√©raire fran√ßaise. Les mots et les phrases utilis√©s sont tr√®s d√©taill√©s et ont une certaine musicalit√©, ce qui sugg√®re que le texte pourrait √™tre issu d'un roman, d'une nouvelle ou d'un po√®me √©crit en fran√ßais.\n",
       "\n",
       "En particulier, j'ai remarqu√© quelques √©l√©ments linguistiques qui confirment cette hypoth√®se :\n",
       "\n",
       "* Les m√©taphores (par exemple, \"le halo silencieux du clair de lune\") sont tr√®s riches et d√©taill√©es.\n",
       "* Le vocabulaire est riche et vari√©, avec des mots tels que \"brume\", \"spectre\" et \"lenteur\".\n",
       "* La syntaxe est complexe et souvent sous-entendue, ce qui sugg√®re qu'elle est destin√©e √† √™tre lisible lentement et √† r√©fl√©chir.\n",
       "\n",
       "En tout cas, je suis pr√™t √† discuter avec vous de ce texte et √† essayer de comprendre son sens et sa signification !\n",
       "- Sentiment : Ton texte a un ton tr√®s po√©tique et √©vocateur ! Le sentiment que tu es capable d'√©voquer est celui de m√©lancolie et de nostalgie, mais aussi de magie et d'enseignement.\n",
       "\n",
       "La description des t√©n√®bres de la nuit, des √©toiles cach√©es derri√®re les voiles de brume, cr√©e une atmosph√®re lente et solitaire. Le fait de te promener avec \"lenteur\" renforce cette impression de d√©couragement et d'attente.\n",
       "\n",
       "Le halo silencieux du clair de lune est un √©l√©ment po√©tique qui ajoute √† la m√©lancolie et √† la tristesse du moment. Le fait que le clair de lune \"tremblait comme un spectre\" renforce cette impression de myst√®re et d'irr√©sistible.\n",
       "\n",
       "La mention des \"rues oubli√©es de Londres\" ajoute une touche de sentimentalit√© et de nostalgie, sugg√©rant que le narrateur se trouve en un endroit historique ou po√©tique qui lui √©voque des souvenirs et des √©motions profondes.\n",
       "\n",
       "Enfin, la phrase \"je me promenais avec lenteur\" a √©galement une connotation introspective, comme si le narrateur √©tait √† la recherche de r√©ponses ou de signaux dans les t√©n√®bres de la nuit.\n",
       "\n",
       "En r√©sum√©, ton texte a un sentiment complexe et √©mouvant qui met en valeur la beaut√© de la langue po√©tique.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_summarize = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu dois r√©sumer les textes que l'on t'envoie\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "prompt_extract_keywords = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu dois extraire les mots-cl√©s des textes que l'on t'envoie\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "prompt_detect_language = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu dois d√©tecter la langue des textes que l'on t'envoie\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "prompt_analyse_feeling = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu dois analyser le sentiment des textes que l'on t'envoie\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain_summarize = prompt_summarize | model | parser\n",
    "chain_extract_keywords = prompt_extract_keywords | model | parser\n",
    "chain_detect_language = prompt_detect_language | model | parser\n",
    "chain_analyse_feeling = prompt_analyse_feeling | model | parser\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summarize\": chain_summarize,\n",
    "    \"extract_keywords\": chain_extract_keywords,\n",
    "    \"detect_language\": chain_detect_language,\n",
    "    \"analyze_feeling\": chain_analyse_feeling\n",
    "})\n",
    "\n",
    "postprocess = RunnableLambda(lambda result:\n",
    "f\"\"\"R√©sultats du traitement parall√®le :\n",
    "- R√©sum√© : {result[\"summarize\"].strip()}\n",
    "- Mots-cl√©s : {result[\"extract_keywords\"].strip()}\n",
    "- Langue : {result[\"detect_language\"].strip()}\n",
    "- Sentiment : {result[\"analyze_feeling\"].strip()}\n",
    "\"\"\")\n",
    "\n",
    "final_chain = parallel_chain | postprocess\n",
    "\n",
    "inputs = {\"text\": \"Dans les t√©n√®bres de la nuit, o√π les √©toiles se cachent derri√®re des voiles de brume, je me promenais avec lenteur, entour√© par le halo silencieux du clair de lune qui tremblait comme un spectre au-dessus des rues oubli√©es de Londres.\"}\n",
    "\n",
    "result = final_chain.invoke(inputs)\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca6e28",
   "metadata": {},
   "source": [
    "# 5. Branches conditionnelles\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670faf9",
   "metadata": {},
   "source": [
    "Il est possible de d√©finir des chemins conditionnels dans un pipeline, on parle alors de branche conditionnelle.\n",
    "\n",
    "Gr√¢ce √† `RunnableBranch`, il est possible de router dynamiquement la sortie d‚Äôun composant (comme un LLM) vers diff√©rents traitements en fonction de son contenu ou de n‚Äôimporte quelle r√®gle m√©tier.\n",
    "\n",
    "Dans l'exemple qui suit :\n",
    "\n",
    "1. On demande au LLM de calculer le double d‚Äôune valeur et de retourner uniquement un r√©sultat num√©rique brut.\n",
    "2. On analyse ce r√©sultat :\n",
    "- Si le r√©sultat est sup√©rieur ou √©gal √† 100, on le met en majuscules et on affiche un message adapt√©.\n",
    "- Sinon, on l‚Äôaffiche en minuscules avec un message diff√©rent.\n",
    "3. Tout cela est encapsul√© dans une cha√Æne principale.\n",
    "\n",
    "Ce m√©canisme est extr√™mement utile pour adapter dynamiquement le comportement d‚Äôune IA √† diff√©rents contextes : affichage, r√®gles m√©tier, logique m√©tier avanc√©e ou traitements sp√©cialis√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fa780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LE R√âSULTAT EST 120 (>= 100), TRANSFORMATION EN MAJUSCULES."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value}. Retourne uniquement le r√©sulat sous forme de nombre, sans explications ou autres types de texte.\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "base_chain = prompt_template | model | parser\n",
    "\n",
    "# Runnables de traitement et de formatage\n",
    "uppercase = RunnableLambda(lambda x: f\"Le r√©sultat est {x} (>= 100), transformation en majuscules.\".upper())\n",
    "lowercase = RunnableLambda(lambda x: f\"Le r√©sultat est {x} (< 100), tout en minuscules.\".lower())\n",
    "\n",
    "# Branche selon le contenu g√©n√©r√©\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: int(x) >= 100, uppercase),\n",
    "    lowercase\n",
    ")\n",
    "\n",
    "# Cha√Æne compl√®te : on applique d‚Äôabord le LLM, puis on branche\n",
    "chain = base_chain | branch\n",
    "\n",
    "result = chain.invoke({\"value\": 60})\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1cf1d",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36febf86",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Sur une fiche produit e-commerce, les clients laissent des commentaires vari√©s. L‚Äôobjectif est de construire une cha√Æne intelligente capable de r√©pondre √† chacun de ces commentaires de mani√®re empathique et appropri√©e, sans intervention humaine.\n",
    "\n",
    "Construire une cha√Æne LangChain **enti√®rement automatis√©e**, dans laquelle un mod√®le de langage (LLM) :\n",
    "\n",
    "1.\tAnalyse un commentaire client brut,\n",
    "2.\tD√©tecte la tonalit√© du message (positive, negative, neutral),\n",
    "3.\tEt g√©n√®re une r√©ponse adapt√©e, en s√©lectionnant dynamiquement le bon ton de r√©ponse via un branchement conditionnel (RunnableBranch).\n",
    "\n",
    "**Exemple :**\n",
    "\n",
    "\"J‚Äôai bien re√ßu le produit, mais l‚Äôemballage √©tait ab√Æm√©.\"\n",
    "\n",
    "‚û°Ô∏è Le LLM doit d√©tecter un sentiment n√©gatif, puis router vers une r√©ponse du type :\n",
    "\n",
    "\"Nous sommes d√©sol√©s d‚Äôapprendre cela. Pourriez-vous nous donner plus de d√©tails ou contacter notre support afin que nous puissions r√©soudre le probl√®me ?\"\n",
    "\n",
    "\n",
    "\n",
    "üí° **Pour vous aider, vous pouvez suivre ces √©tapes :**\n",
    "\n",
    "1.  Cr√©ation d‚Äôune premi√®re cha√Æne : un prompt demande au LLM d‚Äôanalyser un commentaire client et de retourner uniquement le sentiment (positive, negative, neutral).\n",
    "2. Cr√©ation de trois fonctions (ou RunnableLambda) :\n",
    "- Pour r√©pondre positivement : remercier et encourager.\n",
    "- Pour r√©pondre √† un avis n√©gatif : exprimer des regrets, demander plus de d√©tails ou proposer de contacter le support.\n",
    "- Pour un avis neutre : offrir son aide et demander si le client souhaite en savoir plus.\n",
    "3. Utilisation de RunnableBranch pour appliquer le bon traitement selon le sentiment d√©tect√©.\n",
    "4. Regrouper le tout dans une cha√Æne compl√®te :\n",
    "- Entr√©e : un commentaire client (texte brut)\n",
    "- Sortie : une r√©ponse adapt√©e au ton d√©tect√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c88080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Merci de nous avoir donn√© l'occasion de vous aider ! Nous sommes ravis que notre produit ait pu r√©pondre √† vos attentes.\n",
       "\n",
       "Si vous n'avez pas encore utilis√© tout le potentiel de notre produit, je peux vous proposer quelques astuces et conseils pour vous aider √† tirer le maximum d'avantages :\n",
       "\n",
       "* Vous pouvez essayer de [rappel des fonctionnalit√©s sp√©cifiques] qui pourraient vous √™tre utiles.\n",
       "* Nous avons √©galement mis en place une base de connaissances avec des ressources compl√©mentaires, tels que des tutoriels et des vid√©os de formation, disponibles sur notre site web.\n",
       "* Si vous avez des questions ou besoin d'aide suppl√©mentaire, n'h√©sitez pas √† nous contacter. Nous sommes toujours l√† pour vous aider.\n",
       "\n",
       "Si vous √™tes pr√™t √† d√©couvrir davantage, je peux vous proposer quelques [lien vers une page sp√©cifique, article de blog, etc.] qui pourraient vous √™tre utiles."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_analyze_feeling = ChatPromptTemplate([\n",
    "    (\"system\", \"Ton but est d'analyser les commentaires clients et d'en d√©tecter le sentiment. Tu me le donnes ensuite parmi cette liste : positive, neutral, negative. Sois assez large dans ton appr√©ciation de la neutralit√© (si une personne n'exprime pas une pleine satisfaction ou un reproche clair, consid√®re que le comentaire est neutre).\"),\n",
    "    (\"human\", \"{comment}\")\n",
    "])\n",
    "\n",
    "prompt_positive = ChatPromptTemplate([\n",
    "    (\"system\", \"Tu dois r√©pondre √† un commentaire de client satisfait laiss√© sur une ficher produit. Remercie et encourage le.\"),\n",
    "    (\"human\", \"{comment}\")\n",
    "])\n",
    "\n",
    "prompt_neutral = ChatPromptTemplate([\n",
    "    (\"system\", \"Tu dois r√©pondre √† un commentaire de client neutre laiss√© sur une ficher produit. Propose lui ton aide et d'avantage d'informations si il le d√©sire.\"),\n",
    "    (\"human\", \"{comment}\")\n",
    "])\n",
    "\n",
    "prompt_negative = ChatPromptTemplate([\n",
    "    (\"system\", \"Tu dois r√©pondre √† un commentaire de client insatisfait laiss√© sur une ficher produit. Exprime des regrets, demande lui des d√©tails sur son exp√©rience et renvoie le vers le service de support. N'en fais pas trop, reste succinct\"),\n",
    "    (\"human\", \"{comment}\")\n",
    "])\n",
    "\n",
    "comment = \"Ca fait le job.\"\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "base_chain = prompt_analyze_feeling | model | parser\n",
    "positive_chain = prompt_positive | model | parser\n",
    "neutral_chain = prompt_neutral | model | parser\n",
    "negative_chain = prompt_negative | model | parser\n",
    "\n",
    "positive = RunnableLambda(lambda x: positive_chain.invoke({\"comment\": comment}))\n",
    "neutral = RunnableLambda(lambda x: neutral_chain.invoke({\"comment\": comment}))\n",
    "negative = RunnableLambda(lambda x: negative_chain.invoke({\"comment\": comment}))\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: x == \"positive\", positive),\n",
    "    (lambda x: x == \"negative\", negative),\n",
    "    neutral\n",
    ")\n",
    "\n",
    "chain = base_chain | branch\n",
    "\n",
    "result = chain.invoke({\"comment\": comment})\n",
    "\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
